# Generative Zero-Shot Composed Image Retrieval

<img width="2328" height="344" alt="image" src="https://github.com/user-attachments/assets/b4a3956c-4526-483e-8512-ba518a2b37d8" />

Zero-Shot Composed Image Retrieval vs. Pseudo Target-Aided Composed Image Retrieval. Conventional ZS-CIR methods map the image latent embedding into the token embedding space by textual inversion. The proposed Pseudo Target-Aided method provide additional information for composed embeddings from pseudo-target images.

## ğŸ“ Cáº¥u TrÃºc Project

```
ComposedImageGen/
â”‚
â”œâ”€â”€ ğŸ“„ Core Scripts
â”‚   â”œâ”€â”€ extract_lincir_feat.py      # TrÃ­ch xuáº¥t composed embeddings tá»« áº£nh vÃ  captions
â”‚   â”œâ”€â”€ test_CIG.py                 # Táº¡o áº£nh composed sá»­ dá»¥ng SDXL
â”‚   â”œâ”€â”€ phi.py                      # Äá»‹nh nghÄ©a mÃ´ hÃ¬nh Phi network
â”‚   â”œâ”€â”€ config.py                   # Quáº£n lÃ½ paths vÃ  cáº¥u hÃ¬nh táº­p trung
â”‚   â””â”€â”€ download_images.py          # Utility Ä‘á»ƒ download áº£nh tá»« URLs
â”‚
â”œâ”€â”€ ğŸ“‚ datasets/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ dataset_utils.py            # Dataset classes (CIRRDataset, FashionIQDataset, ComposedEmbedsDataset)
â”‚   â”‚
â”‚   â””â”€â”€ FashionIQ/                  # FashionIQ dataset
â”‚       â”œâ”€â”€ captions/               # Caption files cho má»—i category
â”‚       â”‚   â”œâ”€â”€ cap.dress.{train|val|test}.json
â”‚       â”‚   â”œâ”€â”€ cap.shirt.{train|val|test}.json
â”‚       â”‚   â””â”€â”€ cap.toptee.{train|val|test}.json
â”‚       â”‚
â”‚       â”œâ”€â”€ image_splits/           # Image split files
â”‚       â”‚   â”œâ”€â”€ split.dress.{train|val|test}.json
â”‚       â”‚   â”œâ”€â”€ split.shirt.{train|val|test}.json
â”‚       â”‚   â””â”€â”€ split.toptee.{train|val|test}.json
â”‚       â”‚
â”‚       â””â”€â”€ images/                 # Image data vÃ  metadata
â”‚           â”œâ”€â”€ convert.py          # Utility Ä‘á»ƒ convert URLs thÃ nh JSON
â”‚           â”œâ”€â”€ dress.json          # URL database cho dress images
â”‚           â”œâ”€â”€ shirt.json          # URL database cho shirt images
â”‚           â”œâ”€â”€ toptee.json         # URL database cho toptee images
â”‚           â””â”€â”€ downloaded/         # [Táº¡o bá»Ÿi download_images.py] áº¢nh Ä‘Ã£ táº£i vá»
â”‚               â”œâ”€â”€ dress/
â”‚               â”œâ”€â”€ shirt/
â”‚               â””â”€â”€ toptee/
â”‚
â”œâ”€â”€ ğŸ“‚ models/
â”‚   â”œâ”€â”€ phi_best.pt                 # Pretrained Phi cho CLIP-ViT-L/14
â”‚   â””â”€â”€ phi_best_giga.pt           # Pretrained Phi cho CLIP-Giga
â”‚
â”œâ”€â”€ ğŸ“‚ outputs/                     # [Táº¡o khi cháº¡y] Káº¿t quáº£ output
â”‚   â”œâ”€â”€ embeddings/                 # Composed embeddings (.pt files)
â”‚   â””â”€â”€ generated_images/           # Generated images tá»« SDXL
â”‚
â”œâ”€â”€ ğŸ“‚ SEARLE_CIG/                  # SEARLE baseline vá»›i CIG
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ data_utils.py
â”‚       â”œâ”€â”€ datasets.py
â”‚       â”œâ”€â”€ encode_with_pseudo_tokens*.py
â”‚       â”œâ”€â”€ generate_test_submission.py
â”‚       â”œâ”€â”€ gpt_phrases_generation.py
â”‚       â”œâ”€â”€ image_concepts_association.py
â”‚       â”œâ”€â”€ oti_inversion.py
â”‚       â”œâ”€â”€ phi.py
â”‚       â”œâ”€â”€ train_phi.py
â”‚       â”œâ”€â”€ utils_feat.py
â”‚       â””â”€â”€ validate.py
â”‚
â”œâ”€â”€ ğŸ“„ requirements.txt             # Python dependencies
â””â”€â”€ ğŸ“„ README.md                    # Documentation nÃ y

```

## ğŸ”„ Data Flow Pipeline

### 1ï¸âƒ£ Data Preparation

```
URLs (JSON files) â†’ Download Script â†’ Local Images â†’ Dataset Loaders
```

### 2ï¸âƒ£ Feature Extraction

```
Images + Captions â†’ CLIP Models â†’ Phi Networks â†’ Composed Embeddings (.pt)
```

### 3ï¸âƒ£ Image Generation

```
Composed Embeddings â†’ SDXL Pipeline â†’ Generated Images
```

### 4ï¸âƒ£ Evaluation

```
Generated Images â†’ SEARLE Baseline â†’ Retrieval Metrics
```

## ğŸ“Š Dataset Details

### FashionIQ Dataset Structure

**Metadata Files (images/):**

```json
// dress.json, shirt.json, toptee.json
[
  {
    "asin": "B00014NGF6",
    "url": "http://ecx.images-amazon.com/images/I/51AERD8SM8L._SY445_.jpg"
  }
]
```

**Caption Files (captions/):**

```json
// cap.{category}.{split}.json
[
  {
    "target": "B008BHCT58",
    "candidate": "B003FGW7MK",
    "captions": ["is solid black with no sleeves", "is black with straps"]
  }
]
```

**Image Split Files (image_splits/):**

```json
// split.{category}.{split}.json
[
    "B00014NGF6",
    "B0006HJ8FU",
    ...
]
```

### CIRR Dataset Structure

```
cirr/
â”œâ”€â”€ captions/
â”‚   â””â”€â”€ cap.rc2.{split}.json
â”œâ”€â”€ test1/                    # Test images
â”‚   â””â”€â”€ {image_id}.png
â”œâ”€â”€ dev/                      # Development images
â”‚   â””â”€â”€ {image_id}.png
â””â”€â”€ ...
```

## ğŸš€ Getting Started

### 1. CÃ i Äáº·t Dependencies

```bash
pip install -r requirements.txt
```

**YÃªu cáº§u chÃ­nh:**

- Python 3.8+
- PyTorch + CUDA
- transformers, diffusers (HuggingFace)
- CLIP (OpenAI)
- PIL, opencv, albumentations

### 2. Download Pre-trained Weights

Download vÃ  Ä‘áº·t vÃ o thÆ° má»¥c `models/`:

- `phi_best.pt` - Phi model cho CLIP-ViT-L/14
- `phi_best_giga.pt` - Phi model cho CLIP-Giga

Link: [Google Drive](https://drive.google.com/drive/folders/1hpIpI0X26ox-uY-QdOPKDKKZlnWkftIA?usp=drive_link)

### 3. Chuáº©n Bá»‹ Dá»¯ Liá»‡u

#### Option A: Download Images tá»« URLs (FashionIQ)

```bash
python download_images.py --dataset fashioniq --categories dress shirt toptee
```

Script sáº½:

- Äá»c URLs tá»« `datasets/FashionIQ/images/{category}.json`
- Download áº£nh vá» `datasets/FashionIQ/images/downloaded/{category}/`
- Xá»­ lÃ½ lá»—i vÃ  retry khi cáº§n

#### Option B: Sá»­ dá»¥ng CIRR Dataset

Download CIRR dataset theo hÆ°á»›ng dáº«n [táº¡i Ä‘Ã¢y](https://github.com/miccunifi/SEARLE/tree/main#data-preparation) vÃ  Ä‘áº·t vÃ o thÆ° má»¥c `datasets/CIRR/`

### 4. Cáº¥u HÃ¬nh Paths

Chá»‰nh sá»­a `config.py` hoáº·c truyá»n arguments khi cháº¡y:

```python
# config.py
DATASET_PATHS = {
    'cirr': './datasets/CIRR',
    'fashioniq': './datasets/FashionIQ'
}

MODEL_PATHS = {
    'phi_vit': './models/phi_best.pt',
    'phi_giga': './models/phi_best_giga.pt',
    'sdxl_checkpoint': './models/sdxl_checkpoint/'
}

OUTPUT_PATHS = {
    'embeddings': './outputs/embeddings/',
    'generated_images': './outputs/generated_images/'
}
```

## ğŸ¯ Usage

### Step 1: TrÃ­ch Xuáº¥t Composed Embeddings

**Cho CIRR Dataset:**

```bash
python extract_lincir_feat.py \
    --dataset cirr \
    --text_embeddings_dir ./outputs/embeddings/cirr_test/
```

**Cho FashionIQ Dataset:**

```bash
python extract_lincir_feat.py \
    --dataset fashioniq \
    --text_embeddings_dir ./outputs/embeddings/fashioniq_test/
```

**Output:**

- Táº¡o file `.pt` cho má»—i pair trong `text_embeddings_dir/`
- Má»—i file chá»©a: `conditioning`, `conditioning2`, `pooled`, `pooled2`

### Step 2: Táº¡o Composed Images vá»›i SDXL

```bash
python test_CIG.py \
    --text_embeddings_dir ./outputs/embeddings/cirr_test/ \
    --dataset_dir ./datasets/CIRR \
    --model_path ./models/sdxl_checkpoint/ \
    --save_path ./outputs/generated_images/cirr/ \
    --batch_size 4 \
    --height 512 \
    --width 512 \
    --steps 50 \
    --seed 1600
```

**Parameters:**

- `--text_embeddings_dir`: ThÆ° má»¥c chá»©a embeddings tá»« Step 1
- `--dataset_dir`: ThÆ° má»¥c dataset gá»‘c
- `--model_path`: Path Ä‘áº¿n SDXL checkpoint
- `--save_path`: NÆ¡i lÆ°u áº£nh generated
- `--brightness_thresh`: NgÆ°á»¡ng brightness Ä‘á»ƒ filter áº£nh
- `--max_retries`: Sá»‘ láº§n retry náº¿u áº£nh quÃ¡ tá»‘i

### Step 3: Evaluation vá»›i SEARLE

```bash
cd SEARLE_CIG
python src/generate_test_submission.py \
    --submission-name cirr_sdxl_b32 \
    --eval-type searle \
    --dataset cirr \
    --dataset-path ../datasets/CIRR \
    --generated-image-dir ../outputs/generated_images/cirr/
```

## ğŸ§© Components Chi Tiáº¿t

### 1. `phi.py` - Phi Network

```python
class Phi(nn.Module):
    """
    Textual Inversion Phi network.
    Chuyá»ƒn Ä‘á»•i visual features thÃ nh pseudo-token embeddings.

    Architecture: Linear â†’ GELU â†’ Dropout â†’ Linear â†’ GELU â†’ Dropout â†’ Linear
    """
```

### 2. `extract_lincir_feat.py` - Feature Extraction

**Chá»©c nÄƒng:**

- Load CLIP models (ViT-L/14 vÃ  Giga)
- Load Phi networks
- Xá»­ lÃ½ reference images + relative captions
- Táº¡o composed embeddings cho SDXL

**Models Ä‘Æ°á»£c sá»­ dá»¥ng:**

- `openai/clip-vit-large-patch14`
- `Geonmo/CLIP-Giga-config-fixed`

### 3. `test_CIG.py` - Image Generation

**Chá»©c nÄƒng:**

- Load SDXL pipeline
- Batch processing embeddings
- Generate images vá»›i retry mechanism
- Filter theo brightness threshold

### 4. `dataset_utils.py` - Dataset Loaders

#### CIRRDataset

```python
Returns:
    {
        'reference_image': Tensor,
        'relative_caption': str,
        'pairid': str
    }
```

#### FashionIQDataset

```python
Returns:
    {
        'reference_image': Tensor,
        'relative_caption': str
    }
```

#### ComposedEmbedsDataset

```python
Returns:
    {
        'pairid': str,
        'prompt_embeds': Tensor [seq_len, d1+d2],
        'pooled2': Tensor [d2]
    }
```

## âš ï¸ LÆ°u Ã Quan Trá»ng

### 1. Image URLs vs Local Files

- **FashionIQ**: Máº·c Ä‘á»‹nh chá»©a URLs, cáº§n download vá» local
- **CIRR**: Dataset cÃ³ sáºµn local images
- Code Ä‘Ã£ Ä‘Æ°á»£c update Ä‘á»ƒ há»— trá»£ cáº£ 2 modes

### 2. GPU Memory Requirements

- CLIP models: ~2GB VRAM
- SDXL pipeline: ~8GB VRAM
- Tá»•ng khuyáº¿n nghá»‹: GPU vá»›i Ã­t nháº¥t 12GB VRAM

### 3. Storage Requirements

- FashionIQ images: ~50GB (sau khi download)
- CIRR images: ~30GB
- Generated images: Depends on test set size

### 4. Hardcoded Paths

CÃ¡c paths máº·c Ä‘á»‹nh cáº§n Ä‘Æ°á»£c update:

```python
# âŒ CÅ©
dataset_path = "/path/to/cirr/dataset"

# âœ… Má»›i
dataset_path = args.dataset_path or "./datasets/CIRR"
```

## ğŸ”§ Troubleshooting

### Lá»—i: "Cannot load image from URL"

**Giáº£i phÃ¡p:** Run `download_images.py` Ä‘á»ƒ táº£i áº£nh vá» local trÆ°á»›c

### Lá»—i: "CUDA out of memory"

**Giáº£i phÃ¡p:**

- Giáº£m `--batch_size`
- DÃ¹ng `torch_dtype=torch.float16`
- Close cÃ¡c process khÃ¡c sá»­ dá»¥ng GPU

### Lá»—i: "Pretrained model not found"

**Giáº£i phÃ¡p:** Download models vÃ  Ä‘áº·t Ä‘Ãºng vÃ o thÆ° má»¥c `models/`

### áº¢nh generated quÃ¡ tá»‘i

**Giáº£i phÃ¡p:**

- TÄƒng `--max_retries`
- Giáº£m `--brightness_thresh`
- Thá»­ seeds khÃ¡c nhau

## ğŸ“ˆ Performance Tips

1. **Parallel Processing:** TÄƒng `--num_workers` trong DataLoader
2. **Batch Size:** TÄƒng batch_size náº¿u GPU memory Ä‘á»§
3. **Mixed Precision:** DÃ¹ng `torch.float16` cho inference
4. **Caching:** CLIP models cache vÃ o `./cache/` tá»± Ä‘á»™ng

## ğŸ”¥ Updates

- [x] Pretrained weights
- [x] Inference code
- [x] Updated README vá»›i cáº¥u trÃºc chi tiáº¿t
- [x] Download utility cho images
- [x] Fixed hardcoded paths
- [ ] Support more benchmarks and baselines
- [ ] Train code

## ğŸ“š Citation

```bibtex
@inproceedings{wang2025CIG,
  title={Generative zero-shot composed image retrieval},
  author={Wang, Lan and Ao, Wei and Boddeti, Vishnu Naresh and Lim, Sernam},
  booktitle={Proceedings of the Computer Vision and Pattern Recognition Conference},
  year={2025}
}
```

## ğŸ™ Acknowledgements

This project builds upon the following repositories:

- [SEARLE](https://github.com/miccunifi/SEARLE/tree/main)
- [lincir](https://github.com/navervision/lincir)

I am grateful to the authors and contributors of these projects for making their work available to the community.
